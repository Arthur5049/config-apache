#spark-defaults
# https://livebook.manning.com/book/spark-in-action/chapter-11/65

# spark.master                 spark://master:7077
spark.master                   spark://${SPARK_MASTER_NAME}:${SPARK_MASTER_PORT}
# spark.master yarn

spark.executor.cores             4
spark.cores.max                  4

spark.ui.killEnabled	         true
spark.eventLog.enabled           false
# spark.eventLog.dir               hdfs://namenode:8021/directory
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.driver.memory              5g
spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

# spark.ui.view.acls.groups=groupname
# spark.ui.view.acls=usernameâ€‹


spark.default.parallelism 12
spark.executor.instances 2

spark.executor.memory 2g
spark.driver.memory 2g
spark.yarn.am.memory 2g

spark.hadoop.datanucleus.autoCreateSchema=true
spark.hadoop.datanucleus.fixedDatastore=false

#local => /opt/bigdata/migracao/sparkShell.conf
# spark-shell --properties-file C:\Users\\projects\\sparkShell.conf
# alias sshell="spark-shell --master spark://192.168.99.131:7077"
# alias sshell="SPARK_MAJOR_VERSION=2; spark-shell --properties-file sparkShell.conf"
# beeline -u \"jdbc:hive2://192.168.99.127:10000/default\"
# sudo chown -R neto:hadoop metastore_db

#aws
# spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.s3a.access.key=
# spark.hadoop.fs.s3a.secret.key=/eMmH1


# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
# spark.blockManager.port 38000
# spark.broadcast.port 38001
# spark.driver.port 38002
# spark.executor.port 38003
# spark.fileserver.port 38004
# spark.replClassServer.port 38005
# spark.app.name "UsoCluster"
# spark.master yarn
# spark.yarn.queue Qualidade
# spark.eventLog.dir hdfs://server/opt/ingestao/decode_sha2/logs/
# spark.history.fs.logDirectory hdfs://server/opt/ingestao/decode_sha2/logs/
# spark.spark.sql.broadcastTimeout 36000
# spark.sql.codegen.aggregate.map.twolevel.enable false
# spark.sql.caseSensitive false
# spark.shuffle.service.enabled true
# spark.dynamicAllocation.enabled true
# spark.dynamicAllocation.initialExecutors 3
# spark.dynamicAllocation.minExecutors 3
# spark.dynamicAllocation.maxExecutors 25000
# spark.executor.instances 3
# spark.executor.cores 32
# spark.driver.cores 3
# spark.executor.memory 5G
# spark.yarn.executor.memoryOverhead 500
# spark.driver.memory 200G
# spark.yarn.driver.memoryOverhead 20000
# spark.ui.port 4142
# spark.shuffle.compress true
# spark.hadoop.yarn.resourcemanager.webapp.address docker.local:8088
# spark.driver.maxResultSize 10g
# val sparkSession = SparkSession.builder.master("local").appName("sdf")
# .config("spark.sql.warehouse.dir", "/user/hive/warehouse")
# .config("hive.metastore.uris", "thrift://localhost9083").enableHiveSupport().getOrCreate()
# spark.master spark://192.168.99.101:7077
# spark.master spark://localhost:7077
# spark.yarn.queue default
# spark.yarn.archive ""
# spark.yarn.jars ""
# spark.sql.catalogImplementation hive
# spark.sql.warehouse.dir /user/hive/warehouse
# spark.hive.metastore.uris thrift://docker.local:9083
# spark.sql.hive.hiveserver2.jdbc.url jdbc:hive2://docker.local:10000
# spark.sql.hive.hiveserver2.jdbc.url thrift://docker.local:9083 
# spark.datasource.hive.warehouse.load.staging.dir /tmp
# spark.datasource.hive.warehouse.metastoreUri thrift://docker.local:9083
# spark.security.credentials.hiveserver2.enabled false
# spark.eventLog.dir hdfs://localhost:9000/var/logs/
# spark.history.fs.logDirectory hdfs://localhost:9000/var/logs/
# spark.dynamicAllocation.enabled true    
# spark.dynamicAllocation.minExecutors 1
# spark.executor.cores 4
# spark.driver.cores 4
# spark.executor.memory 6G
# spark.yarn.executor.memoryOverhead 2000
# spark.driver.memory 6G
# spark.yarn.driver.memoryOverhead 2000
# spark.hadoop.yarn.resourcemanager.webapp.address 192.168.99.101:8088
# spark.hadoop.yarn.resourcemanager.webapp.address 192.168.99.101:7077


# https://apache.googlesource.com/spark/+/master/docs/running-on-yarn.md
# scala> spark.conf.getAll.foreach(println(_))
# (spark.sql.warehouse.dir,/user/hive/warehouse)
# (spark.driver.host,localhost)
# (spark.ui.port,4142)
# (spark.driver.port,56926)
# (spark.shuffle.service.enabled,true)
# (spark.yarn.queue,default)
# (spark.repl.class.uri,spark://localhost:56926/classes)
# (spark.jars,)
# (spark.repl.class.outputDir,/private/var/folders/46/9b_xx0jn3gx3h13nsgvp6kc00000gn/T/spark-026c4648-a0b9-4658-8e25-77c464a20540/repl-8c7b2eaf-6678-44ae-9b1b-e8d34e48d0c8)
# (spark.app.name,Spark shell)
# (spark.driver.memory,1G)
# (spark.ui.showConsoleProgress,true)
# (spark.executor.id,driver)
# (spark.hadoop.yarn.resourcemanager.webapp.address,localhost:8088)
# (spark.submit.deployMode,client)
# (spark.master,spark://192.168.99.100:7077)
# (spark.executor.memory,1G)
# (spark.home,/opt/spark-2.3.0)
# (spark.dynamicAllocation.enabled,true)
# (spark.sql.catalogImplementation,hive)
# (spark.executor.cores,1)
# (spark.app.id,app-20181120005120-0010)
# spark.app.name "UsoCluster"
# spark.yarn.queue hdqueue
# spark.hadoop.yarn.resourcemanager.webapp.address server.redecorp.br:8088
# spark.conf.set("spark.hadoop.yarn.resourcemanager.webapp.address","192.168.99.100:8032")
# spark.conf.set("spark.sql.catalogImplementation", "hive")
# spark.conf.set("spark.app.name","UsoCluster")
# spark.conf.set("appName","DQ-Quantitativo")
# spark.conf.set("spark.yarn.queue","faturamento")
# spark.conf.set("spark.sql.broadcastTimeout","36000")
# spark.conf.set("spark.sql.codegen.aggregate.map.twolevel.enable","false")
# spark.conf.set("spark.sql.caseSensitive","false")
# spark.conf.set("spark.shuffle.service.enabled","true")
# spark.conf.set("spark.dynamicAllocation.enabled","true")
# spark.conf.set("spark.dynamicAllocation.initialExecutors","3")
# spark.conf.set("spark.dynamicAllocation.minExecutors","3")
# spark.conf.set("spark.dynamicAllocation.maxExecutors","25000")
# spark.conf.set("spark.executor.instances","3")
# spark.conf.set("spark.executor.cores","32")
# spark.conf.set("spark.executor.memory","5G")
# spark.conf.set("spark.yarn.executor.memoryOverhead","50000")
# spark.conf.set("spark.driver.memory","200G")
# spark.conf.set("spark.yarn.driver.memoryOverhead","20000")
# spark.conf.set("spark.scheduler.mode","FIFO")
# spark.conf.set("spark.ui.port","4142")
# spark.conf.set("spark.shuffle.compress","true")
# spark.conf.set("spark.hadoop.yarn.resourcemanager.webapp.address","server.redecorp.br:8088")
# spark.conf.set("spark.master","yarn")
# spark.conf.set("hive.execution.engine","spark")
# spark.conf.set("hive.merge.mapredfiles","true")
# spark.conf.set("hive.merge.size.per.task","128000000")
# spark.conf.set("hive.merge.smallfiles.avgsize","128000000")
# spark.conf.set("hive.auto.convert.join","true")
# spark.conf.set("hive.auto.convert.sortmerge.join","true")
# spark.conf.set("hive.exec.dynamic.partition","true")
# spark.conf.set("hive.exec.dynamic.partition.mode","nonstrict")
# spark.conf.set("hive.exec.max.dynamic.partitions","500000")
# spark.conf.set("hive.vectorized.execution.enabled","true")
# spark.conf.set("hive.vectorized.execution.reduce.enabled","true")
# spark.conf.set("hive.cbo.enable","true")
# spark.conf.set("hive.compute.query.using.stats","true")
# spark.conf.set("hive.stats.fetch.column.stats","true")
# spark.conf.set("hive.stats.fetch.partition.stats","true")
# spark.conf.set("tez.queue.name","faturamento")
# spark.conf.set("mapreduce.job.queuename","faturamento")
# spark.conf.set("appName","Decode_anon_TV")
# spark.conf.set("spark.yarn.queue","hdqueue")
# spark.conf.set("spark.driver.cores","3")
# spark.conf.set("spark.yarn.executor.memoryOverhead","500")
# spark.conf.set("tez.queue.name","hdqueue")
# spark.conf.set("mapreduce.job.queuename","hdqueue")
