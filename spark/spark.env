#!/bin/bash

#spark
export SPARK_HOME=$OPT_PATH/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

export SPARK_LOCAL_IP=127.0.0.1
# SPARK_LOCAL_IP=192.168.99.131
# SPARK_LOCAL_IP=localhost

# export SPARK_MASTER_HOST=srv-based
# export SPARK_MASTER_HOST=127.0.0.1
# export SPARK_WORKER_HOST=127.0.0.1

export SPARK_WORKER_CORES=4
export SPARK_WORKER_INSTANCES=3

# export SPARK_WORKER_WEBUI_PORT=8084

#spark
# SPARK_CONF_DIR=/usr/hdp/current/spark2-client/conf/
# SPARK_HOME=/usr/hdp/current/spark2-client
# SPARK_MAJOR_VERSION=2


# spark-shell --master yarn --executor-memory 80G --executor-cores 24 --num-executors 56 --driver-memory 80G --queue Desenvolvimento
#master
# http://172.25.102.68:8080/
#history
# http://localhost:18080/

#install spark3
# https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/

# export SPARK_HOME=~/hadoop/spark-3.1.1
# export SPARK_HOME=/usr/local/Cellar/apache-spark/3.1.2/libexec
# export PATH=$SPARK_HOME/bin:$PATH
# Configure Spark to use Hadoop classpath
# export SPARK_DIST_CLASSPATH=$(hadoop classpath)

# ======================================= alias =============================================
# alias sshell="spark-shell --master spark://192.168.99.131:7077"
# --properties-file ~/projects/77-config/config/sparkShell.conf 
# alias sshell="SPARK_MAJOR_VERSION=2; spark-shell \
#     --jars \
#         $OPT_PATH/projects/_roo/consolidate-spark/target/scala-2.12/consolidate-spark-assembly-0.1.jar"


PATH_APP=${PROJECTS_ROOT}/${NAME_PROJECT}
CONFIG=${PATH_APP}/src/main/resources/"application.conf"
PATH_JARS=${PATH_APP}/"target/scala-2.12"/${NAME_PROJECT}"-assembly-0.1.jar"

#scala
alias sshell="spark-shell --master spark://${SPARK_MASTER_NAME}:${SPARK_MASTER_PORT} \
    --driver-memory 2G --executor-memory 2G --executor-cores 2 \
    --packages \
        org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,com.crealytics:spark-excel_2.12:3.2.0_0.16.0,io.delta:delta-core_2.12:1.1.0,com.typesafe:config:1.4.1 \
    --jars ${APACHE_CONF}/hive/jars/mysql-connector-java-8.0.17.jar,${PATH_JARS} \
    --driver-java-options='-Dconfig.file='${CONFIG} \
    --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' \
    --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog'"


#test-xls
# val df =spark.read.format("com.crealytics.spark.excel").option("header", "true").option("dataAddress", "'DPCache_m3_2'!A1:Q10").schema(fuelSchema).load("/test/vendas-combustiveis-m3.xls")
# df.filter(col("ESTADO")==="AMAZONAS").agg(sum(col("Jan"))).show


#python
alias pyshell="pyspark --master spark://${SPARK_MASTER_NAME}:${SPARK_MASTER_PORT} \
    --driver-memory 2G --executor-memory 2G --executor-cores 2 \
    --packages io.delta:delta-core_2.12:1.1.0 \
    --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' \
    --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog'"



# alias spark="docker exec -it spark-master /bin/bash"
# alias cluster="docker exec -it -u cloudera claro-m2m_cloudera_1 /bin/bash"
# alias sshell="docker exec -it spark-master /spark/bin/spark-shell \
#         --packages \
#             com.rabbitmq:amqp-client:5.10.0"
# --jars \
#     /library/jars/spark3-transform-assembly-1.0.26.jar \
# spark3-transform-assembly-1.0.22.jar
#     --packages \
#         org.apache.livy:livy-api:0.7.0-incubating,com.github.junqueira:test-library_2.12:1.0.18,org.apache.livy:livy-client-http:0.7.0-incubating"
# com.github.mrpowers:spark-stringmetric_2.12:0.3.0, \
# alias sshell="spark-shell --properties-file ~/projects/77-config/config/sparkShell.conf"
#env-spark
# http://localhost:4040/environment/
# ~/projects/scala/anonymize-spark/config/app_test.conf \
# prioridade
# 	NameNode
# 	NodeManager
# 	JobHistoryServer
# 	ResourceManager

#install
# wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz
# sudo tar xvf spark-3.0.1-bin-hadoop2.7.tgz -C /opt/spark
# =================================================== #k8s ========================================================
alias spark8s="kubectl exec -it spark-555f658f9-kscbq --namespace spark /bin/bash"

#livy
# alias livy="kubectl --namespace spark port-forward spark-livy-0 8998"
alias livy="kubectl --namespace livy port-forward livy-0 8998"
# alias livy="kubectl --namespace spark port-forward livy-0 8998"

#history
# alias history="kubectl --namespace spark port-forward historyserver-d87bf9fd9-4p8b8 18080"
# alias history="kubectl --namespace spark port-forward historyserver 18080"
# ====================================================== #docker ======================================================
# alias spark="docker exec -it spark-master /bin/bash"
# alias sshell="docker exec -it spark-master /spark/bin/spark-shell \
#         --packages \
#             com.rabbitmq:amqp-client:5.10.0"

#spark-shell  \
#  --jars /usr/hdp/3.0.1.0-183/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.0.1.0-183.jar \
#  --conf spark.security.credentials.hiveserver2.enabled=false \
#  --conf spark.sql.hive.hiveserver2.jdbc.url=jdbc:hive2://localhost:10000 \
#  --conf spark.datasource.hive.warehouse.load.staging.dir=/tmp \
#  --conf spark.datasource.hive.warehouse.metastoreUri=thrift://localhost:9083

# test-libray
#     /Volumes/F/projects/scala/test-library/target/scala-2.12/test-library_2.12-0.1.0-SNAPSHOT.jar \
#     ~/projects/scala/test-library/target/scala-2.12/test-library_2.12-0.1.0-SNAPSHOT.jar \

# alias sshell="spark-shell \
#     --properties-file \
#         ~/projects/_roo/77-config/config/apache/sparkShell.conf \
#     --jars \
#         /Volumes/F/projects/scala/test-library/target/scala-2.12/test-library-assembly-0.1.0-SNAPSHOT.jar \
#         ~/projects/scala/anonymize-spark/target/scala-2.11/anonymize-assembly-1.0.jar, \
#         ~/projects/_roo/77-config/config/jars/hadoop-aws-2.8.1.jar, \
#         ~/projects/_roo/77-config/config/jars/aws-java-sdk-1.11.179.jar \
#         com.databricks:spark-avro_2.11:4.0.0, \
#         org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5, \
#         org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.5, \
#         org.mongodb.spark:mongo-spark-connector_2.11:2.4.2"
# --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.1.0"
# com.databricks:spark-avro_2.10:3.0.0"
# com.databricks:spark-avro_2.10:2.0.1"
# com.databricks:spark-avro_2.11:3.2.0"

# =================================================== #enviroment ====================================================
# LIVY_POD_NAME=livy-0
# LIVY_POD_NAMESPACE=livy
# SPARK_MAJOR_VERSION=2
# # SPARK_HOME=/opt/spark/spark-2.4.6-bin-hadoop2.7
# SPARK_HOME=/usr/local/Cellar/apache-spark/3.0.1/libexec
# PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# PYTHONPATH=/usr/local/Cellar/apache-spark/2.4.4/libexec/python/:$PYTHONP$

# source ~/.bash_profile
### in conf/spark-env.sh ###
# If 'hadoop' binary is on your PATH
# SPARK_DIST_CLASSPATH=$(hadoop classpath)
# With explicit path to 'hadoop' binary
# SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)
# Passing a Hadoop configuration directory
# SPARK_DIST_CLASSPATH=$(hadoop --config /path/to/configs classpath)
#SPARK_SUBMIT_OPTS="-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005"
#SPARK_HOME="/opt/spark-2.2.1"
#PATH=$SPARK_HOME/bin:$PATH
# PYTHONPATH=/usr/local/bin/python3.6
# PYSPARK_PYTHON=/usr/local/bin/python3.7
# SPARK_SSH_FOREGROUND=true
# SPARK_HOME="/opt/spark-2.3.0"
# SPARK_INSTALL=/usr/local/Cellar/apache-spark/2.4.5
# SPARK_INSTALL=/usr/local/Cellar/apache-spark/3.0.0
# SPARK_HOME=$SPARK_INSTALL/libexec
# SPARK_INSTALL=/usr/local/Cellar/apache-spark/2.4.3/spark-2.4.3-bin-hadoop2.6
# SPARK_HOME=$SPARK_INSTALL
# SPARK_PATH=$SPARK_HOME
# PATH=$SPARK_HOME/bin:$PATH
